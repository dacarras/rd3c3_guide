# Applied Examples

## Summary

-   We use data from TIMSS 2019, from the student background questionnaire.
    -   In particular we are using student responses to the items of the scale "Sense of School Belonging", from Chile and England.
    -   The data file includes 4115 students from Chile, and 3365 students from England, from 8th grade
    -   We are using a tailor-made data file where we include specific clustering and survey design variables:
        -   id_i = unique id number for students
        -   id_j = unique id number for schools
        -   id_s = unique id number for stratification factors (i.e., JKZONES)
        -   id_k = unique id number for country samples
        -   ws = scale survey weights to a constant of 1000
        -   [`data_ex1.rds`](https://www.dropbox.com/scl/fi/aivrd2rw2lhw5o7ly06jn/data_ex1.rds?rlkey=3clglj3fdlh5o6myv3q8et9xz&dl=1)
-   To install the library, a user can use the following code, to download the development version of the library:

```{r, echo = TRUE, eval = FALSE, warning=FALSE, message = FALSE}
#| code-fold: FALSE

# -----------------------------------------------
# install rd3c3
# -----------------------------------------------

devtools::install_github(
  'dacarras/rd3c3',
  force = TRUE
)

```

-   We include three applied examples:
    -   Invariance Analysis
        -   Model based invariance analysis
        -   Alignment method analsyis
    -   Item report analysis
        -   The item report analysis includes a larger set of analysis, besides Model based invariance analysis and Alignment method analsyis, such as item descriptives, missing data analysis, parallel analysis and reliability analysis among others.

## Invariance analysis

### Model based invariance analysis

Model based invariance analysis includes five model specifications:

-   **pooled** is a graded response model with probit link, including survey design variables (i.e., stratification factors, primary sampling unit and student survey weights) (e.g., Stapleton, 2013). *Pooled* a single measurement model, fitted onto pooled sample of cases were sampling weights have been scale to a common constant so including countries contributes equally to estimates (Gonzalez, 2012).

-   **strict** is a multigroup graded response model, where all response model parameters are held equal between compared groups, beside latent means for groups (Tse et al., 2024), while including the scale factors as part of the model.

-   **scalar** is a multigroup graded response model, where all response model parameters are held equal among groups, with the exemption of scale factors. It follows model specifications described in Svetina et al. (2020, proposition 7), where scale factors are fix to one for the reference group, and let to vary free on the rest of the groups in the comparison.

-   **threshold** is a multigroup graded response model, where thresholds are held common among the compared countries (i.e., threshold invariance). Is the baseline model for a model building sequence for assessing model-based measurement invariance (Wu & Estabrook, 2016). It follows model specifications described in Svetina et al. (2020, proposition 4).

-   **configural** is a multigroup graded response model, where all measurement model parameters are freely estimated among the compare groups, while holding latent factor means fix to zero, and factor variances fixed to one across groups.

Simulations studies from Rutkowski & Svetina (2017) with the graded response model with probit link and a larger amount of compare groups (e.g., 10, 20) suggest that RMSEA \< .055 serves as a rule of thumb to select well-fitting measurement models with invariant parameters among compared groups.

To fit the following models we use as inputs:

-   data_responses [`data_ex1.rds`](https://www.dropbox.com/scl/fi/n4ioht1v4zg7gos2a4esj/data_ex1.rds?rlkey=0lxeaq44jfmaf0ub3g3w1kvyv&st=4lysfefy&dl=1)
-   scale_info = [`guideline_scale_info_example.xlsx`](https://www.dropbox.com/scl/fi/ujsl6fpvsbjlor2ovc6pg/guideline_scale_info_example.xlsx?rlkey=yx2mwdxt6lr0exh24ddjkexdo&st=j2446g7r&dl=1)
-   scale_id = 1
-   and is using the functions:
    -   `rd3c3::fit_grm2` for the pooled model
    -   `rd3c3::fit_grm2_m01_strict` for the strict model
    -   `rd3c3::fit_grm2_m02_scalar` for the scalar model
    -   `rd3c3::fit_grm2_m03_threshold` for the threshold model
    -   `rd3c3::fit_grm2_m04_config` for the base model (i.e., descriptive model)

In the following section we include code (*folded*) to produce the invariance model fit indexes table (see Table 1).

```{r, echo = TRUE, eval = TRUE, warning=FALSE, message = FALSE}
#| code-fold: true

#------------------------------------------------------------------------------
# define objects
#------------------------------------------------------------------------------

# -----------------------------------------------
# scale id
# -----------------------------------------------

rd3c3::silent(library(dplyr))

# -----------------------------------------------
# scale id
# -----------------------------------------------

scale_id <- 1

# -----------------------------------------------
# scales info
# -----------------------------------------------

scales_data <- readxl::read_xlsx(
               'guideline_scale_info_example.xlsx', 
               sheet = 'scales_data'
               )

# -----------------------------------------------
# data file
# -----------------------------------------------

data_file <- scales_data %>%
dplyr::filter(scale_num == scale_id) %>%
dplyr::select(data_file) %>%
unique() %>%
dplyr::pull() 

# -----------------------------------------------
# response matrix
# -----------------------------------------------

data_responses <- readRDS(data_file) %>%
mutate(grp = paste0(COUNTRY)) %>%
mutate(grp = as.numeric(as.factor(COUNTRY))) %>%
mutate(grp_name = paste0(COUNTRY))

#------------------------------------------------------------------------------
# response models
#------------------------------------------------------------------------------

# -----------------------------------------------
# most centered
# -----------------------------------------------

grp_centered <- 'CHL'

# -----------------------------------------------
# pooled
# -----------------------------------------------

inv_0 <- rd3c3::silent(
             rd3c3::fit_grm2(
           data = data_responses,
           scale_num  = scale_id,
             scale_info = scales_data
              )
             )

# -----------------------------------------------
# strict
# -----------------------------------------------

inv_1 <- rd3c3::silent(
           rd3c3::fit_grm2_m01_strict(
           data = data_responses, 
           scale_num = scale_id, 
           scale_info = scales_data,
           grp_var = 'id_k',
           grp_txt = 'grp_name',
           grp_ref = grp_centered
           )
           )

# -----------------------------------------------
# scalar
# -----------------------------------------------

inv_2 <- rd3c3::silent(
           rd3c3::fit_grm2_m02_scalar(
           data = data_responses, 
           scale_num = scale_id, 
           scale_info = scales_data,
           grp_var = 'id_k',
           grp_txt = 'grp_name',
           grp_ref = grp_centered
           )
           )

# -----------------------------------------------
# threshold
# -----------------------------------------------

inv_3 <- rd3c3::silent(
           rd3c3::fit_grm2_m03_threshold(
           data = data_responses, 
           scale_num = scale_id, 
           scale_info = scales_data,
           grp_var = 'id_k',
           grp_txt = 'grp_name',
           grp_ref = grp_centered
           )
           )

# -----------------------------------------------
# configural
# -----------------------------------------------

inv_4 <- rd3c3::silent(
           rd3c3::fit_grm2_m04_config(
           data = data_responses, 
           scale_num = scale_id, 
           scale_info = scales_data,
           grp_var = 'id_k',
           grp_txt = 'grp_name',
           grp_ref = grp_centered
           )
           )

# -----------------------------------------------
# retrieve fit indexes per model
# -----------------------------------------------

fit_0 <- rd3c3::get_inv_fit(inv_0, model_name = 'pooled')
fit_1 <- rd3c3::get_inv_fit(inv_1, model_name = 'strict')
fit_2 <- rd3c3::get_inv_fit(inv_2, model_name = 'scalar')
fit_3 <- rd3c3::get_inv_fit(inv_3, model_name = 'threshold')
fit_4 <- rd3c3::get_inv_fit(inv_4, model_name = 'configural')

# -----------------------------------------------
# general table
# -----------------------------------------------

fit_table <- dplyr::bind_rows(
             dplyr::select(fit_0, model, RMSEA, CFI, TLI, SRMR, x2, df, p_val),
             dplyr::select(fit_1, model, RMSEA, CFI, TLI, SRMR, x2, df, p_val),
             dplyr::select(fit_2, model, RMSEA, CFI, TLI, SRMR, x2, df, p_val),
             dplyr::select(fit_3, model, RMSEA, CFI, TLI, SRMR, x2, df, p_val),
             dplyr::select(fit_4, model, RMSEA, CFI, TLI, SRMR, x2, df, p_val)
             )


# -----------------------------------------------
# model fit
# -----------------------------------------------

fit_table %>%
knitr::kable(., 
  digits = c(0,3,2,2,2,2,0,2),
    caption = 'Table 1: invariance model fit indexes between compared groups'
  )

```

> Note: pooled = is the measurement model fitted onto the pooled sample. strict = is a multigroup measurement model with common thresholds, common loadings, and a common scale. This latent variable model suffices mean score comparisons (Tse et al., 2024). scalar = is a multigroup measurement model with common thresholds, common loadings, and free scales for each item. This model supports latent mean comparisons (Tse et al., 2024). threshold = is a multigroup measurement model with common thresholds. configural = is a multigroup descriptive model where all measurement model parameter are free to vary. Metric model specification is not identified under the graded response models (Wu & Estabrook, 2016), thus metric specifications are not included. A RMSEA of .055 or less has been found to be good threshold for fit for graded response models with many groups of 10 or 20 compared groups (see Rutkowski & Svetina, 2017).

### Alignment

The alignment method is optimizing for the least discrepant measurement model parameters among the compared groups. Is fitting a graded response model with probit link, and using the most optimal group as a reference. We are using the statement `ALIGNMENT = FIXED(*);` within Mplus for these purposes. We rely on the *Measurement invariance explorer* (https://github.com/MaksimRudnev/MIE.package) to retrieve aligment results.

The following code (*folded*) is used as inputs:

-   data_responses [`data_ex1.rds`](https://www.dropbox.com/scl/fi/n4ioht1v4zg7gos2a4esj/data_ex1.rds?rlkey=0lxeaq44jfmaf0ub3g3w1kvyv&st=4lysfefy&dl=1)
-   scale_info = [`guideline_scale_info_example.xlsx`](https://www.dropbox.com/scl/fi/ujsl6fpvsbjlor2ovc6pg/guideline_scale_info_example.xlsx?rlkey=yx2mwdxt6lr0exh24ddjkexdo&st=j2446g7r&dl=1)
-   scale_id = 1
-   and is using the function `rd3c3::fit_grm2_align_wlsmv()` to run an alignment method analysis

```{r, echo = TRUE, eval = TRUE, warning=FALSE, message = FALSE}
#| code-fold: true

#------------------------------------------------------------------------------
# define objects
#------------------------------------------------------------------------------

# -----------------------------------------------
# scale id
# -----------------------------------------------

rd3c3::silent(library(dplyr))

# -----------------------------------------------
# scale id
# -----------------------------------------------

scale_id <- 1

# -----------------------------------------------
# scales info
# -----------------------------------------------

scales_data <- readxl::read_xlsx(
               'guideline_scale_info_example.xlsx', 
               sheet = 'scales_data'
               )

# -----------------------------------------------
# data file
# -----------------------------------------------

data_file <- scales_data %>%
dplyr::filter(scale_num == scale_id) %>%
dplyr::select(data_file) %>%
unique() %>%
dplyr::pull() 

# -----------------------------------------------
# response matrix
# -----------------------------------------------

data_responses <- readRDS(data_file) %>%
mutate(grp = paste0(COUNTRY)) %>%
mutate(grp = as.numeric(as.factor(COUNTRY))) %>%
mutate(grp_name = paste0(COUNTRY))

#------------------------------------------------------------------------------
# alignment
#------------------------------------------------------------------------------

# -----------------------------------------------
# aligned
# -----------------------------------------------


fitted_align <- rd3c3::silent(
rd3c3::fit_grm2_align_wlsmv(
data = data_responses, 
scale_num = scale_id, 
scale_info = scales_data)
)

# -----------------------------------------------
# retrieve output
# -----------------------------------------------

scale_file <- scales_data %>%
dplyr::filter(scale_num == scale_id) %>%
dplyr::select(mplus_file) %>%
unique() %>%
dplyr::pull() 


alignment_out <- MIE::extractAlignment(paste0(scale_file,'_align.out'), silent = TRUE)

# -----------------------------------------------
# display
# -----------------------------------------------

alignment_table <- alignment_out$summary %>%
                   tibble::rownames_to_column("terms") %>%
                   tibble::as_tibble() %>%
                   rename(
                   term        = 1,
                   a_par       = 2,
                   R2          = 3,
                   n_inv       = 4,
                   n_dis       = 5,
                   inv_grp     = 6,
                   dis_grp     = 7
                   ) %>%
                   mutate(type = case_when(
                   stringr::str_detect(term, 'Threshold') ~ 'tau',
                   stringr::str_detect(term, 'Loadings')  ~ 'lambda'
                   )) %>%              
                   mutate(term = stringr::str_replace(term, '\\$', '_')) %>%
                   mutate(term = stringr::str_replace(term, 'Threshold', '')) %>%
                   mutate(term = stringr::str_replace(term, 'Loadings', '')) %>%
                   mutate(term = stringr::str_replace(term, 'ETA by ', '')) %>%
                   dplyr::select(
                   type, term, a_par, R2, n_inv, n_dis, inv_grp, dis_grp)

# -----------------------------------------------
# display
# -----------------------------------------------

alignment_table %>%
knitr::kable(., 
  digits = 2,
    caption = 'Table 2: alignment comparisons'
)

```

## Item report analysis

In the following section we include a **template** example, to produce **item analysis reports**. This template includes:

-   **Scale description**
    -   a presentation of the name of the collection of items (i.e., the scale name)
    -   a presentation of items as these where presented to the participants
    -   a table with the item text, with the public data file names, and the shortened variable names
-   **Analysis of responses**
    -   descriptives
    -   missing data descriptives
-   **Response model**
    -   dimensionality analysis via parallel analysis for ordinal indicators (Lubbe, 2019)
    -   response model parameters for a graded response model
    -   reliability analysis
    -   item person maps
-   **Item analysis**
    -   item test correlation
    -   item fit based on partial credit model
-   **Comparability**
    -   model based measurement invariance
    -   alignment analysis of GRM among groups

The following figure depicts an overview of the generated report.

```{r echo = FALSE, out.width = '100%', fig.retina = 2, fig.cap = 'Figure 5: overview of a dynamic item report'}

# -----------------------------------------------
# instrument
# -----------------------------------------------

knitr::include_graphics(paste0('figure_5_overview.png'))

```

To produce this examplary report the user needs as inputs:

-   data_responses [`data_ex1.rds`](https://www.dropbox.com/scl/fi/n4ioht1v4zg7gos2a4esj/data_ex1.rds?rlkey=0lxeaq44jfmaf0ub3g3w1kvyv&st=4lysfefy&dl=1)
-   scale_info = [`guideline_scale_info_example.xlsx`](https://www.dropbox.com/scl/fi/ujsl6fpvsbjlor2ovc6pg/guideline_scale_info_example.xlsx?rlkey=yx2mwdxt6lr0exh24ddjkexdo&st=j2446g7r&dl=1)
-   scale_id = 1
-   the template [`guideline_item_report_example.rmd`](https://www.dropbox.com/scl/fi/iyu8p1jdxclzfu3y220n7/guideline_item_report_example.rmd?rlkey=vamga7s12hcsi4wa9pyrv6awz&st=kzbbnc9y&dl=1)
    -   and the word template [`report_template.docx`](https://www.dropbox.com/scl/fi/n4c5uf6f498z9v6qvg4xx/report_template.docx?rlkey=yc0b2914kn2clsxrebiz5etgt&st=bn2kl7ca&dl=1)

The end product of this procedure can be inspected in the following file [`guideline_item_report_example.docx`](https://www.dropbox.com/scl/fi/h7erj79fqhxs4rz6gv9b7/guideline_item_report_example.docx?rlkey=x6c816u9b87a5mudbzmpbhpdl&st=zgyid2hm&dl=1)

## Additional examples

The current examples are just toy examples, to illustrate the basic capabilities of the library. We include two other data response files. One example with three countries, where is possible to see that three countries do not obtain strict invariance (example with "Sense of School Belonging" for three countries). And a third example of a template-based workflow, where we procede with a full fledge item analysis report including all participating countries (example with the "Bullying Scale" for all participating countries).

-   Example with "Sense of School Belonging" for three countries
    -   data: [`data_example.rds`](https://www.dropbox.com/scl/fi/1z7lt5f0jea7thz9xgbct/data_example.rds?rlkey=sl46n4owlgbjiguk1ou6sefym&st=dyfnlr5s&dl=1)
    -   code template: [`template_example_2.rmd`](https://www.dropbox.com/scl/fi/1ujerb30tijxz3t3tgrme/template_example_2.rmd?rlkey=kwiirvitq3eo0ptav0znoc7o9&st=hvig6j6k&dl=1)
        -   word template: [`report_template.docx`](https://www.dropbox.com/scl/fi/n4c5uf6f498z9v6qvg4xx/report_template.docx?rlkey=yc0b2914kn2clsxrebiz5etgt&st=bn2kl7ca&dl=1)
    -   resulting report: [`template_example_2.docx`](https://www.dropbox.com/scl/fi/sis0bcfhvt0r5tl75tt8y/template_example_2.docx?rlkey=cd88m6p0qeuv5g3khcwtp32vk&st=u9ht2d8q&dl=1)
-   Example with "Bullying Scale" for all participating countries
    -   data[`survey_1_g8.rds`](https://www.dropbox.com/scl/fi/2r759ij7gc1w8xeq34zv8/survey_1_g8.rds?rlkey=6ghx1wonw2mfbvy1p4l14ojgf&st=b9a170iz&dl=1)
    -   code template: [`template_example_3.rmd`](https://www.dropbox.com/scl/fi/s3hl05llbdedi79f6nha3/template_example_3.rmd?rlkey=67y5wl45gg7k4snbf14lfrrga&st=f8k6d76n&dl=1)
        -   word template: [`report_template.docx`](https://www.dropbox.com/scl/fi/n4c5uf6f498z9v6qvg4xx/report_template.docx?rlkey=yc0b2914kn2clsxrebiz5etgt&st=bn2kl7ca&dl=1)
    -   resulting report [`template_example_3.docx`](https://www.dropbox.com/scl/fi/ps7o8z917cnswtswzkc7u/template_example_3.docx?rlkey=txtazzoz0ennstqurdgbrjlz4&st=mlc980t5&dl=1).

All example files can be downladed from the following [link](https://www.dropbox.com/scl/fo/vn8n2bv3zpi76gpdcf4ii/ABD4kDEMNH5nX6u23HBOZY0?rlkey=xmyi11t4w5t60n0goohdey2fz&st=o3qd1ck6&dl=1).

## References

Gonzalez, E. J. (2012). Rescaling sampling weights and selecting mini-samples from large-scale assessment databases. IERI Monograph Series Issues and Methodologies in Large-Scale Assessments, 5, 115–134.

Tse, W. W. Y., Lai, M. H. C., & Zhang, Y. (2024). Does strict invariance matter? Valid group mean comparisons with ordered-categorical items. Behavior Research Methods, 56(4), 3117–3139. https://doi.org/10.3758/s13428-023-02247-6

Stapleton, L. M. (2013). Incorporating Sampling Weights into Single- and Multilevel Analyses. In L. Rutkowski, M. von Davier, & D. Rutkowski (Eds.), Handbook of International Large scale Assessment: background, technical issues, and methods of data analysis (pp. 363–388). Chapman and Hall/CRC.

Svetina, D., Rutkowski, L., & Rutkowski, D. (2020). Multiple-Group Invariance with Categorical Outcomes Using Updated Guidelines: An Illustration Using Mplus and the lavaan/semTools Packages. Structural Equation Modeling: A Multidisciplinary Journal, 27(1), 111–130. https://doi.org/10.1080/10705511.2019.1602776

Wu, M., Tam, H. P., & Jen, T.-H. (2016). Educational Measurement for Applied Researchers. Springer Singapore. https://doi.org/10.1007/978-981-10-3302-5

Rutkowski, L., & Svetina, D. (2017). Measurement Invariance in International Surveys: Categorical Indicators and Fit Measure Performance. Applied Measurement in Education, 30(1). https://doi.org/10.1080/08957347.2016.1243540
